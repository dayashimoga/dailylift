# Technical Requirements Document (TRD)

## Project Name: Automated Programmatic SEO Directory
**Tagline:** A 100% Zero-Maintenance, Automated Passive Income Machine  
**Architecture Theme:** "Boring", Static, $0 Operations  
**Business Model:** Programmatic SEO -> Ads/Affiliates -> Profit  

---

## 1. Executive Summary

This project aims to create a fully automated, programmatic SEO directory (e.g., "The Ultimate Directory of Open APIs" or "Free AI Tools Database") that runs completely on autopilot. Leveraging the existing **DailyLift** architecture, this project will automatically fetch data from a public source, generate thousands of static, SEO-optimized HTML pages, publish them via GitHub Actions & Netlify, and promote them automatically on social media. The operational cost will be exactly $0, with zero manual intervention required after the initial setup.

---

## 2. System Architecture

The tech stack relies entirely on static file generation to ensure zero database costs, zero server maintenance, and near-infinite scalability.

*   **Hosting & CDN:** Netlify (Free Tier)
*   **Automation & Compute:** GitHub Actions (Free)
*   **Data Storage:** Flat JSON files committed to the repository
*   **Site Generator:** Custom Node.js scripts (Vanilla JS, no heavy frameworks)
*   **Monetization:** Google AdSense (Display Ads) & Affiliate Networks (PartnerStack, Amazon Associates)
*   **Social Media:** Automated Mastodon/Twitter API integration via GitHub Actions

---

## 3. Data Ingestion & Storage

### 3.1 Data Source
The system must pull structured data (JSON or CSV) from a reliable, free source.
*   *Example Sources:* [Public APIs GitHub Repository](https://github.com/public-apis/public-apis), Kaggle datasets, open job board RSS feeds.

### 3.2 Fetch Script (`scripts/fetch-data.js`)
*   A Node.js script responsible for calling the external API or parsing an RSS/CSV file.
*   It must sanitize the data, extract relevant fields (Title, Description, Category, URL, Meta Tags), and save it locally to `data/database.json`.
*   **Error Handling:** If the fetch fails, it must exit gracefully (e.g., `process.exit(0)`) so the CI/CD pipeline does not fail conspicuously. It will simply try again on the next cron schedule.

---

## 4. Static Site Generation (SSG)

### 4.1 Build Script (`scripts/build-directory.js`)
Instead of server-side rendering, all pages must be pre-built.
*   The script reads `data/database.json`.
*   It iterates through the array of items.
*   For each item, it injects the data into a master HTML template (`src/templates/item.html`) using simple string replacement (e.g., `{{ITEM_TITLE}}`, `{{ITEM_DESC}}`).
*   It outputs the generated HTML file into `dist/items/item-slug-name.html`.

### 4.2 Category & Index Pages
*   The script must also group items by category (e.g., `dist/category/finance-apis.html`) and generate an index page containing links to categories or recent items.

### 4.3 Sitemap Generation (`scripts/generate-sitemap.js`)
*   Crucial for Programmatic SEO. The script must dynamically generate `dist/sitemap.xml` containing URLs for every single item and category page generated during the build.

---

## 5. SEO & Performance Requirements

*   **Meta Tags:** Every generated page must have unique `<title>`, `<meta name="description">`, and Open Graph tags injected from the JSON data.
*   **URL Structure:** URLs must be semantic and keyword-rich (e.g., `yoursite.com/tool/best-free-weather-api`).
*   **Performance:** 100/100 Lighthouse score required. No external CSS libraries (like Tailwind or Bootstrap) that require heavy compilation; use minimal vanilla CSS.
*   **Structured Data:** Inject JSON-LD schema (e.g., `SoftwareApplication` or `WebSite`) into the `<head>` of item pages to secure rich snippets in Google Search.

---

## 6. Automation Pipelines (GitHub Actions)

### 6.1 Data Refresh Cron (`.github/workflows/data-sync.yml`)
*   **Trigger:** Schedule (e.g., `cron: '0 0 * * 0'` for weekly on Sundays).
*   **Job:**
    1. Checkout repo.
    2. Set up Node.js.
    3. Run `npm run fetch-data`.
    4. Commit changes to `data/database.json` via a bot account.
    5. Push changes to the `main` branch.

### 6.2 Build & Deploy Workflow (Managed by Netlify)
*   Netlify will automatically detect the push generated by the Data Refresh Cron.
*   Netlify runs `npm run build` (which includes generating the thousands of HTML pages and the sitemap).
*   Netlify deploys the `dist/` folder to the live edge nodes.

### 6.3 Social Media Bot (`.github/workflows/social-bot.yml`)
*   **Trigger:** Schedule (e.g., Daily at 12:00 PM UTC).
*   **Job:**
    1. Run `node scripts/post-random-item.js`.
    2. The script picks a random item from `data/database.json`.
    3. It formats a promotional message with relevant hashtags.
    4. It posts the message to Mastodon/Twitter via REST API using repository secrets (`MASTODON_ACCESS_TOKEN`).

---

## 7. Monetization Integration

*   **Display Ads:** A dedicated `<div>` in `src/templates/item.html` (e.g., sidebar and below-content) containing Google AdSense `<ins>` tags.
*   **Affiliate Injection:** If the data source allows, the build script should identify keywords (e.g., "hosting", "AWS") and append your affiliate tracking IDs to outgoing links.
*   **Premium Placements (Gumroad):** A static "Submit a Tool / Pin your Tool" link in the navigation bar pointing to a Gumroad checkout. Fulfillment remains manual for this edge case.

---

## 8. Setup & Integration Instructions (Step-by-Step)

Follow these steps to integrate this architecture into the `h:\boringwebsite` repository:

### Phase 1: Preparation
1.  **Create Directories:**
    *   Create `src/templates/` folder.
    *   Create `data/` folder (if not already strictly used for this).
2.  **Define Templates:**
    *   Create `src/templates/item.html` containing placeholders like `{{title}}`, `{{description}}`, `{{link}}`, `{{category}}`. Make sure it includes the AdSense code snippet placeholders.
    *   Create `src/templates/index.html` for the homepage.

### Phase 2: Core Scripts
1.  **Draft `scripts/fetch-data.js`:**
    *   Write a script using `fetch()` to grab a public JSON dataset (e.g., public APIs or a static JSON file you host elsewhere).
    *   Use `fs.writeFileSync` to save it to `data/database.json`.
2.  **Draft `scripts/build-directory.js`:**
    *   Use `fs.readFileSync` to read the template and `data/database.json`.
    *   Loop through the JSON array.
    *   Use `.replaceAll()` to swap placeholders with actual data.
    *   Write the resulting HTML strings to the `dist/items/` directory.
3.  **Draft `scripts/generate-sitemap.js`:**
    *   Iterate through the generated files and build a standard XML sitemap string.
    *   Write to `dist/sitemap.xml`.

### Phase 3: Workflow Updates
1.  **Update `package.json`:**
    *   Change the `build` script to explicitly sequence the new tasks: `"build": "node scripts/build-directory.js && node scripts/generate-sitemap.js"`.
2.  **Add Data Sync GitHub Action:**
    *   Create `.github/workflows/data-sync.yml`.
    *   Configure it to run `node scripts/fetch-data.js`, run `git add data/database.json`, `git commit`, and `git push`.
3.  **Update Social Media Script:**
    *   Modify existing `scripts/post-social.js` to pick a random item from the new `database.json` rather than the old daily quotes array.

### Phase 4: Deployment
1.  Push all changes to the `main` branch.
2.  Netlify will automatically rebuild the site.
3.  Verify the generated pages exist by visiting `/items/your-item-slug`.
4.  Submit the new `https://yourdomain.com/sitemap.xml` to Google Search Console to initiate mass indexing.
